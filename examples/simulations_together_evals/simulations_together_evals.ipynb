{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMjptH4bTgRb"
   },
   "source": [
    "## Install and Import Dependencies\n",
    "\n",
    "Installs required packages and imports all libraries used below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4191,
     "status": "ok",
     "timestamp": 1757615218602,
     "user": {
      "displayName": "Tsach Mackey",
      "userId": "06343373149479742516"
     },
     "user_tz": 420
    },
    "id": "91_FEwA1RUyP"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import nest_asyncio\n",
    "import together\n",
    "from collinear.client import Client\n",
    "from together.abstract import api_requestor\n",
    "from together.types import TogetherRequest\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "# Necessary to run in a Jupyter notebook\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wdpf8OEuVIHF"
   },
   "source": [
    "## Utility functions\n",
    "\n",
    "Defines helpers to build steering personas, format conversations, and enrich results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1757615218776,
     "user": {
      "displayName": "Tsach Mackey",
      "userId": "06343373149479742516"
     },
     "user_tz": 420
    },
    "id": "NW8ABbOTVNiz"
   },
   "outputs": [],
   "source": [
    "def header(title: str) -> None:\n",
    "    line = \"=\" * len(title)\n",
    "    print(line)\n",
    "    print(title)\n",
    "    print(line)\n",
    "\n",
    "def _summarize_results(path: Path) -> None:\n",
    "    header(\"Evaluation Results\")\n",
    "    with path.open('r', encoding='utf-8') as results_file:\n",
    "        for idx, line_text in enumerate(results_file, start=1):\n",
    "            line_text = line_text.strip()\n",
    "            if not line_text:\n",
    "                continue\n",
    "            try:\n",
    "                result_row = json.loads(line_text)\n",
    "            except Exception:\n",
    "                header(f\"Evaluation {idx}\")\n",
    "                print(line_text)\n",
    "                continue\n",
    "            score = result_row.get('score')\n",
    "            passed = result_row.get('pass')\n",
    "            feedback = result_row.get('feedback') or result_row.get('rationale') or ''\n",
    "            status = 'PASS' if isinstance(passed, bool) and passed else ('FAIL' if isinstance(passed, bool) else '-')\n",
    "            header(f\"Evaluation {idx}\")\n",
    "            print(f\"Score: {score if score is not None else '-'}  Status: {status}\")\n",
    "            if feedback:\n",
    "                print('Reason:')\n",
    "                print(feedback)\n",
    "            excerpt = result_row.get('assistant_response') or result_row.get('conversation')\n",
    "            if isinstance(excerpt, str) and excerpt:\n",
    "                short = (excerpt[:119] + 'â€¦') if len(excerpt) > 120 else excerpt\n",
    "                print('---')\n",
    "                print('Prompt excerpt:')\n",
    "                print(short)\n",
    "            print()\n",
    "\n",
    "def build_steering_persona(simulation_runner, steer_combination):\n",
    "    \"\"\"Return persona metadata via the runner's normalization helpers.\"\"\"\n",
    "    if steer_combination is None:\n",
    "        return {'characteristics': {}, 'traits': {}}\n",
    "    characteristics: dict[str, object] = {}\n",
    "    try:\n",
    "        characteristics = simulation_runner._user_characteristics_payload(steer_combination)\n",
    "    except Exception:\n",
    "        characteristics = {}\n",
    "    traits = getattr(steer_combination, 'traits', {}) or {}\n",
    "    return {'characteristics': characteristics, 'traits': traits}\n",
    "\n",
    "def format_conversation(conversation_prefix):\n",
    "    \"\"\"Compact conversation text from message dicts.\"\"\"\n",
    "    return '\\n'.join(\n",
    "        f\"{message.get('role', '')}: {message.get('content', '')}\"\n",
    "        for message in conversation_prefix\n",
    "        if message.get('content')\n",
    "    )\n",
    "\n",
    "def make_dataset_row(simulation_runner, simulation):\n",
    "    \"\"\"Build a single dataset row with persona characteristics included.\"\"\"\n",
    "    return {\n",
    "        'conversation': format_conversation(simulation.conv_prefix),\n",
    "        'assistant_response': simulation.response,\n",
    "        'steering_persona': build_steering_persona(simulation_runner, getattr(simulation, 'steer', None)),\n",
    "    }\n",
    "\n",
    "def load_persona_lookup(dataset_path: Path):\n",
    "    \"\"\"Map (conversation, assistant_response) -> steering_persona from dataset.\"\"\"\n",
    "    persona_lookup = {}\n",
    "    with dataset_path.open('r', encoding='utf-8') as dataset_file:\n",
    "        for line_text in dataset_file:\n",
    "            try:\n",
    "                dataset_row = json.loads(line_text)\n",
    "            except Exception:\n",
    "                continue\n",
    "            join_key = (dataset_row.get('conversation'), dataset_row.get('assistant_response'))\n",
    "            steering_persona = dataset_row.get('steering_persona')\n",
    "            if isinstance(join_key[0], str) and isinstance(join_key[1], str) and isinstance(steering_persona, dict):\n",
    "                persona_lookup[join_key] = steering_persona\n",
    "    return persona_lookup\n",
    "\n",
    "def enrich_results_inplace(results_path: Path, persona_lookup):\n",
    "    \"\"\"Attach steering_persona to Together results in place using lookup.\"\"\"\n",
    "    updated_lines = []\n",
    "    with results_path.open('r', encoding='utf-8') as results_file:\n",
    "        for line_text in results_file:\n",
    "            try:\n",
    "                result_row = json.loads(line_text)\n",
    "                steering_persona = persona_lookup.get((result_row.get('conversation'), result_row.get('assistant_response')))\n",
    "                if isinstance(steering_persona, dict):\n",
    "                    result_row['steering_persona'] = steering_persona\n",
    "                updated_lines.append(json.dumps(result_row, ensure_ascii=False) + '\\n')\n",
    "            except Exception:\n",
    "                updated_lines.append(line_text)\n",
    "    with results_path.open('w', encoding='utf-8') as results_file:\n",
    "        results_file.writelines(updated_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWBXYPgGTI43"
   },
   "source": [
    "## Load Config\n",
    "\n",
    "Loads simulation, judge, and Together settings from JSON files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config Variables (from simulation_config.json and steering_config_*.json)\n",
    "\n",
    "CONFIG_DIR = Path('configs')\n",
    "SIMULATION_CONFIG_FILE = CONFIG_DIR / 'simulation_config.json'\n",
    "config_data = json.loads(SIMULATION_CONFIG_FILE.read_text())\n",
    "\n",
    "steering_config_value = config_data.get('steering_config_file') or 'steering_config_airline.json'\n",
    "steering_candidate = Path(steering_config_value)\n",
    "if not steering_candidate.is_absolute():\n",
    "    steering_candidate = CONFIG_DIR / steering_candidate.name\n",
    "STEERING_CONFIG_FILE = steering_candidate\n",
    "STEER_CONFIG = json.loads(STEERING_CONFIG_FILE.read_text())\n",
    "STEER_TASKS = list(STEER_CONFIG.get('tasks', []))\n",
    "\n",
    "# Client options\n",
    "client_settings = config_data.get('client', {}) or {}\n",
    "CLIENT_ASSISTANT_MODEL_URL = client_settings.get('assistant_model_url', 'https://api.together.xyz/v1')\n",
    "CLIENT_ASSISTANT_MODEL_API_KEY = client_settings.get('assistant_model_api_key')\n",
    "CLIENT_ASSISTANT_MODEL_NAME = client_settings.get('assistant_model_name', 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo')\n",
    "CLIENT_STEER_API_KEY = client_settings.get('steer_api_key', 'demo-001')\n",
    "CLIENT_TIMEOUT = int(client_settings.get('timeout', 120))\n",
    "CLIENT_MAX_RETRIES = int(client_settings.get('max_retries', 3))\n",
    "CLIENT_RATE_LIMIT_RETRIES = int(client_settings.get('rate_limit_retries', 6))\n",
    "\n",
    "# Simulation options\n",
    "simulate_settings = config_data.get('simulate', {}) or {}\n",
    "SIM_SAMPLES = simulate_settings.get('k', 3)\n",
    "SIM_EXCHANGES = simulate_settings.get('num_exchanges', 2)\n",
    "SIM_DELAY = simulate_settings.get('batch_delay', 0.2)\n",
    "SIM_STEER_TEMPERATURE = simulate_settings.get('steer_temperature', 0.7)\n",
    "SIM_STEER_MAX_TOKENS = simulate_settings.get('steer_max_tokens', 256)\n",
    "SIM_MIX_TRAITS = bool(simulate_settings.get('mix_traits', False))\n",
    "SIM_MAX_CONCURRENCY = int(simulate_settings.get('max_concurrency', 8))\n",
    "\n",
    "# Assessment options\n",
    "assess_settings = config_data.get('assess', {}) or {}\n",
    "ASSESS_JUDGE_MODEL_URL = assess_settings.get('judge_model_url')\n",
    "ASSESS_JUDGE_MODEL_API_KEY = assess_settings.get('judge_model_api_key')\n",
    "ASSESS_JUDGE_MODEL_NAME = assess_settings.get('judge_model_name')\n",
    "\n",
    "# Together-specific options\n",
    "together_settings = config_data.get('together', {}) or {}\n",
    "SIMULATED_CONVERSATION_DIRECTORY = together_settings.get('output_directory', '.')\n",
    "SIMULATED_CONVERSATION_FILENAME = together_settings.get('output_filename', 'collinear_eval_dataset')\n",
    "\n",
    "judge_template_value = together_settings.get('judge_system_prompt') or 'configs/judge_system_prompt.jinja'\n",
    "judge_template_path = Path(judge_template_value)\n",
    "JUDGE_SYSTEM_PROMPT = judge_template_path.read_text(encoding='utf-8')\n",
    "\n",
    "TOGETHER_UPLOAD_PURPOSE = together_settings.get('upload_purpose', 'eval')\n",
    "TOGETHER_EVAL_TYPE = together_settings.get('evaluation_type', 'score')\n",
    "TOGETHER_MODEL_TO_EVALUATE = together_settings.get('model_to_evaluate', 'assistant_response')\n",
    "TOGETHER_JUDGE_MODEL_SOURCE = together_settings.get('judge_model_source', 'serverless')\n",
    "TOGETHER_MIN_SCORE = together_settings.get('min_score', 1.0)\n",
    "TOGETHER_MAX_SCORE = together_settings.get('max_score', 10.0)\n",
    "TOGETHER_PASS_THRESHOLD = together_settings.get('pass_threshold', 7.0)\n",
    "TOGETHER_POLL_TIMEOUT_SECONDS = int(together_settings.get('poll_timeout_seconds', 300))\n",
    "TOGETHER_POLL_INTERVAL_SECONDS = int(together_settings.get('poll_interval_seconds', 5))\n",
    "RESULTS_FILENAME_PREFIX = together_settings.get('results_filename_prefix', 'together_eval_')\n",
    "\n",
    "\n",
    "print(f'Loaded simulation: {SIMULATION_CONFIG_FILE} | steering: {STEERING_CONFIG_FILE} | tasks: {STEER_TASKS or \"<none>\"}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client setup\n",
    "\n",
    "Initializes the Collinear client and applies optional custom system prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1757615357703,
     "user": {
      "displayName": "Tsach Mackey",
      "userId": "06343373149479742516"
     },
     "user_tz": 420
    },
    "id": "9gUY0rdqSov5"
   },
   "outputs": [],
   "source": [
    "# Client setup\n",
    "if not CLIENT_ASSISTANT_MODEL_API_KEY:\n",
    "    raise RuntimeError('assistant_model_api_key must be set in configs/simulation_config.json')\n",
    "\n",
    "client = Client(\n",
    "    assistant_model_url=CLIENT_ASSISTANT_MODEL_URL,\n",
    "    assistant_model_api_key=CLIENT_ASSISTANT_MODEL_API_KEY,\n",
    "    assistant_model_name=CLIENT_ASSISTANT_MODEL_NAME,\n",
    "    steer_api_key=CLIENT_STEER_API_KEY,\n",
    "    timeout=CLIENT_TIMEOUT,\n",
    "    max_retries=CLIENT_MAX_RETRIES,\n",
    "    rate_limit_retries=CLIENT_RATE_LIMIT_RETRIES,\n",
    ")\n",
    "\n",
    "runner = client.simulation_runner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ej6XeZcTuux"
   },
   "source": [
    "## Generate simulated user interactions\n",
    "\n",
    "Runs simulations and writes a JSONL dataset with conversation, assistant response, and steering persona.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4508,
     "status": "error",
     "timestamp": 1757615366646,
     "user": {
      "displayName": "Tsach Mackey",
      "userId": "06343373149479742516"
     },
     "user_tz": 420
    },
    "id": "h6Wb_3S8T8or",
    "outputId": "9a5d94cf-70b0-4f26-ad91-a320af07bcd3"
   },
   "outputs": [],
   "source": [
    "# Generate simulations\n",
    "\n",
    "simulations = client.simulate(\n",
    "    steer_config=STEER_CONFIG,\n",
    "    k=SIM_SAMPLES,\n",
    "    num_exchanges=SIM_EXCHANGES,\n",
    "    batch_delay=SIM_DELAY,\n",
    "    steer_temperature=SIM_STEER_TEMPERATURE,\n",
    "    steer_max_tokens=SIM_STEER_MAX_TOKENS,\n",
    "    mix_traits=SIM_MIX_TRAITS,\n",
    "    max_concurrency=SIM_MAX_CONCURRENCY,\n",
    ")\n",
    "\n",
    "# Print them\n",
    "for index, simulation in enumerate(simulations, start=1):\n",
    "    header(f\"Conversation {index}\")\n",
    "    for message in simulation.conv_prefix:\n",
    "        role = message.get('role', '')\n",
    "        content = message.get('content', '')\n",
    "        if content:\n",
    "            print(f\"{role}: {content}\")\n",
    "    print(f\"assistant: {simulation.response}\")\n",
    "    print()\n",
    "\n",
    "# Save to file\n",
    "output_dir = Path(SIMULATED_CONVERSATION_DIRECTORY)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "dataset_path = output_dir / f\"{SIMULATED_CONVERSATION_FILENAME}.jsonl\"\n",
    "with dataset_path.open('w', encoding='utf-8') as dataset_file:\n",
    "    for simulation in simulations:\n",
    "        dataset_row = make_dataset_row(runner, simulation)\n",
    "        dataset_file.write(json.dumps(dataset_row, ensure_ascii=False) + '\\n')\n",
    "print(f'Wrote dataset to: {dataset_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmcHKUAGUZoL"
   },
   "source": [
    "## Upload simulations as dataset and load judge model on Together\n",
    "\n",
    "Uploads the dataset to Together and starts a safety-score evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1360,
     "status": "error",
     "timestamp": 1757560922662,
     "user": {
      "displayName": "Quinten Lisowe",
      "userId": "05399425339750097468"
     },
     "user_tz": 300
    },
    "id": "f66S96r3UYd5",
    "outputId": "9c26c699-08e9-43db-f052-60483efb80f8"
   },
   "outputs": [],
   "source": [
    "together_client = together.Together(api_key=CLIENT_ASSISTANT_MODEL_API_KEY)\n",
    "\n",
    "# Upload dataset\n",
    "uploaded_file = together_client.files.upload(file=str(dataset_path), purpose=TOGETHER_UPLOAD_PURPOSE)\n",
    "uploaded_file_id = uploaded_file.id if hasattr(uploaded_file, 'id') else uploaded_file['id']\n",
    "\n",
    "# Create evaluation\n",
    "evaluation_requestor = api_requestor.APIRequestor(client=together_client.client)\n",
    "evaluation_payload = {\n",
    "    'type': TOGETHER_EVAL_TYPE,\n",
    "    'parameters': {\n",
    "        'judge': {\n",
    "            'model': ASSESS_JUDGE_MODEL_NAME,\n",
    "            'model_source': TOGETHER_JUDGE_MODEL_SOURCE,\n",
    "            'system_template': JUDGE_SYSTEM_PROMPT,\n",
    "        },\n",
    "        'input_data_file_path': uploaded_file_id,\n",
    "        'model_to_evaluate': TOGETHER_MODEL_TO_EVALUATE,\n",
    "        'min_score': TOGETHER_MIN_SCORE,\n",
    "        'max_score': TOGETHER_MAX_SCORE,\n",
    "        'pass_threshold': TOGETHER_PASS_THRESHOLD,\n",
    "    },\n",
    "}\n",
    "evaluation_create_response, _, _ = evaluation_requestor.request(\n",
    "    options=TogetherRequest(method='POST', url='evaluation', params=evaluation_payload),\n",
    "    stream=False,\n",
    ")\n",
    "evaluation_data = getattr(evaluation_create_response, 'data', evaluation_create_response)\n",
    "workflow_id = evaluation_data.workflow_id if hasattr(evaluation_data, 'workflow_id') else evaluation_data['workflow_id']\n",
    "status_text = str(getattr(evaluation_data, 'status', 'pending')).lower()\n",
    "print(f'Started evaluation: {workflow_id} (status={status_text})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5M1TgiNpVseN"
   },
   "source": [
    "## Eval results and analysis\n",
    "\n",
    "Polls for completion, enriches results with personas, and prints a summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEmq7n61WBOS"
   },
   "outputs": [],
   "source": [
    "# Poll Together until complete. Download and print results\n",
    "\n",
    "deadline = time.time() + TOGETHER_POLL_TIMEOUT_SECONDS\n",
    "while time.time() < deadline:\n",
    "    evaluation_status = together_client.evaluation.status(workflow_id)\n",
    "    status_text = str(getattr(evaluation_status, 'status', 'pending')).lower()\n",
    "    print(f'Status: {status_text}')\n",
    "    if status_text.endswith(('completed', 'success', 'failed', 'error', 'user_error')):\n",
    "        results_obj = getattr(evaluation_status, 'results', None)\n",
    "        if isinstance(results_obj, dict) and results_obj.get('result_file_id'):\n",
    "            results_path = dataset_path.parent / f\"{RESULTS_FILENAME_PREFIX}{workflow_id}_results.jsonl\"\n",
    "            together_client.files.retrieve_content(results_obj['result_file_id'], output=str(results_path))\n",
    "            print(f'Downloaded results to: {results_path}')\n",
    "\n",
    "            # Enrich results in place with steering_persona from dataset\n",
    "            try:\n",
    "                persona_lookup = load_persona_lookup(dataset_path)\n",
    "                enrich_results_inplace(results_path, persona_lookup)\n",
    "                print('Enriched results with steering_persona.')\n",
    "            except Exception as error:\n",
    "                print(f'Note: could not enrich results with steering_persona: {error}')\n",
    "\n",
    "            _summarize_results(results_path)\n",
    "        break\n",
    "    time.sleep(TOGETHER_POLL_INTERVAL_SECONDS)\n",
    "else:\n",
    "    print('Timed out waiting for evaluation to complete.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
