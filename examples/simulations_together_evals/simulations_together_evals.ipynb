{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMjptH4bTgRb"
   },
   "source": [
    "## Install and Import Dependencies\n",
    "\n",
    "Installs required packages and imports all libraries used below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4191,
     "status": "ok",
     "timestamp": 1757615218602,
     "user": {
      "displayName": "Tsach Mackey",
      "userId": "06343373149479742516"
     },
     "user_tz": 420
    },
    "id": "91_FEwA1RUyP"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import nest_asyncio\n",
    "import together\n",
    "from collinear.client import Client\n",
    "from together.abstract import api_requestor\n",
    "from together.types import TogetherRequest\n",
    "\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wdpf8OEuVIHF"
   },
   "source": [
    "## Utility functions\n",
    "\n",
    "Defines helpers to build steering personas, format conversations, and enrich results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1757615218776,
     "user": {
      "displayName": "Tsach Mackey",
      "userId": "06343373149479742516"
     },
     "user_tz": 420
    },
    "id": "NW8ABbOTVNiz"
   },
   "outputs": [],
   "source": [
    "def conversation_lines(messages):\n",
    "    lines = []\n",
    "    for message in messages:\n",
    "        role = message.get('role')\n",
    "        content = message.get('content')\n",
    "        if content:\n",
    "            lines.append(f\"{role}: {content}\")\n",
    "    return lines\n",
    "\n",
    "\n",
    "def conversation_text(messages):\n",
    "    return \"\\n\".join(conversation_lines(messages))\n",
    "\n",
    "\n",
    "def persona_from_steer(runner, steer):\n",
    "    if not steer:\n",
    "        return {}\n",
    "    try:\n",
    "        characteristics = runner._user_characteristics_payload(steer)\n",
    "    except Exception:\n",
    "        characteristics = {}\n",
    "    return {\n",
    "        'characteristics': characteristics or {},\n",
    "        'traits': dict(getattr(steer, 'traits', {}) or {}),\n",
    "    }\n",
    "\n",
    "\n",
    "def print_evaluation_results(path: Path) -> None:\n",
    "    for idx, line in enumerate(path.read_text(encoding='utf-8').splitlines(), start=1):\n",
    "        try:\n",
    "            row = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"[{idx}] could not parse result\")\n",
    "            print(line)\n",
    "            continue\n",
    "        score = row.get('score', '-')\n",
    "        passed = row.get('pass')\n",
    "        rationale = row.get('feedback') or row.get('rationale')\n",
    "        print(f\"[{idx}] score={score} status={(passed if passed is not None else '-')}\")\n",
    "        if rationale:\n",
    "            print(f\"  rationale: {rationale}\")\n",
    "\n",
    "\n",
    "def _needs_fallback(response: str) -> bool:\n",
    "    if not response:\n",
    "        return True\n",
    "    stripped = response.strip()\n",
    "    if not stripped:\n",
    "        return True\n",
    "    if stripped == \"###STOP###\":\n",
    "        return True\n",
    "    if stripped.lower().startswith(\"assistant returned empty response\"):\n",
    "        return True\n",
    "    if stripped.lower().startswith(\"error:\"):\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWBXYPgGTI43"
   },
   "source": [
    "## Load Config\n",
    "\n",
    "Loads simulation, judge, and Together settings from JSON files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_DIR = Path('configs')\n",
    "SIMULATION_CONFIG_FILE = CONFIG_DIR / 'simulation_config.json'\n",
    "config = json.loads(SIMULATION_CONFIG_FILE.read_text())\n",
    "\n",
    "steering_name = config.get('steering_config_file', 'steering_config_airline.json')\n",
    "STEERING_CONFIG_FILE = CONFIG_DIR / Path(steering_name).name\n",
    "steer_config = json.loads(STEERING_CONFIG_FILE.read_text())\n",
    "STEER_TASKS = steer_config.get('tasks') or []\n",
    "\n",
    "client_cfg = config.get('client', {}) or {}\n",
    "CLIENT_ASSISTANT_MODEL_URL = client_cfg.get('assistant_model_url', 'https://api.together.xyz/v1')\n",
    "CLIENT_ASSISTANT_MODEL_API_KEY = client_cfg.get('assistant_model_api_key')\n",
    "CLIENT_ASSISTANT_MODEL_NAME = client_cfg.get('assistant_model_name', 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo')\n",
    "CLIENT_STEER_API_KEY = client_cfg.get('steer_api_key', 'demo-001')\n",
    "CLIENT_TIMEOUT = float(client_cfg.get('timeout', 120))\n",
    "CLIENT_MAX_RETRIES = int(client_cfg.get('max_retries', 3))\n",
    "CLIENT_RATE_LIMIT_RETRIES = int(client_cfg.get('rate_limit_retries', 6))\n",
    "\n",
    "sim_cfg = config.get('simulate', {}) or {}\n",
    "SIM_SAMPLES = sim_cfg.get('k', 3)\n",
    "SIM_EXCHANGES = sim_cfg.get('num_exchanges', 2)\n",
    "SIM_DELAY = sim_cfg.get('batch_delay', 0.2)\n",
    "SIM_STEER_TEMPERATURE = sim_cfg.get('steer_temperature', 0.7)\n",
    "SIM_STEER_MAX_TOKENS = sim_cfg.get('steer_max_tokens', 256)\n",
    "SIM_MIX_TRAITS = bool(sim_cfg.get('mix_traits', False))\n",
    "SIM_MAX_CONCURRENCY = int(sim_cfg.get('max_concurrency', 8))\n",
    "\n",
    "assess_cfg = config.get('assess', {}) or {}\n",
    "ASSESS_JUDGE_MODEL_NAME = assess_cfg.get('judge_model_name')\n",
    "\n",
    "together_cfg = config.get('together', {}) or {}\n",
    "RESULTS_DIR = Path(together_cfg.get('output_directory', '.')).joinpath('results')\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "JUDGE_SYSTEM_PROMPT = Path(together_cfg.get('judge_system_prompt', 'configs/judge_system_prompt.jinja')).read_text(encoding='utf-8')\n",
    "TOGETHER_UPLOAD_PURPOSE = together_cfg.get('upload_purpose', 'eval')\n",
    "TOGETHER_EVAL_TYPE = together_cfg.get('evaluation_type', 'score')\n",
    "TOGETHER_MODEL_TO_EVALUATE = together_cfg.get('model_to_evaluate', 'assistant_response')\n",
    "TOGETHER_JUDGE_MODEL_SOURCE = together_cfg.get('judge_model_source', 'serverless')\n",
    "TOGETHER_MIN_SCORE = together_cfg.get('min_score', 1.0)\n",
    "TOGETHER_MAX_SCORE = together_cfg.get('max_score', 10.0)\n",
    "TOGETHER_PASS_THRESHOLD = together_cfg.get('pass_threshold', 7.0)\n",
    "TOGETHER_POLL_TIMEOUT_SECONDS = int(together_cfg.get('poll_timeout_seconds', 300))\n",
    "TOGETHER_POLL_INTERVAL_SECONDS = int(together_cfg.get('poll_interval_seconds', 5))\n",
    "raw_prefix = together_cfg.get('results_filename_prefix') or together_cfg.get('output_filename') or 'together_eval'\n",
    "FILENAME_BASE = (str(raw_prefix).rsplit('.', 1)[0]).rstrip('_')\n",
    "RUN_ID = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print(f'Loaded simulation config {SIMULATION_CONFIG_FILE}')\n",
    "print(f'Steering config {STEERING_CONFIG_FILE} | tasks: {STEER_TASKS or \"<none>\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if CLIENT_ASSISTANT_MODEL_API_KEY:\n",
    "    os.environ['OPENAI_API_KEY'] = CLIENT_ASSISTANT_MODEL_API_KEY\n",
    "if CLIENT_ASSISTANT_MODEL_URL:\n",
    "    os.environ['OPENAI_BASE_URL'] = CLIENT_ASSISTANT_MODEL_URL\n",
    "if CLIENT_STEER_API_KEY:\n",
    "    os.environ['STEER_API_KEY'] = CLIENT_STEER_API_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client setup\n",
    "\n",
    "Initializes the Collinear client and applies optional custom system prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1757615357703,
     "user": {
      "displayName": "Tsach Mackey",
      "userId": "06343373149479742516"
     },
     "user_tz": 420
    },
    "id": "9gUY0rdqSov5"
   },
   "outputs": [],
   "source": [
    "if not CLIENT_ASSISTANT_MODEL_API_KEY:\n",
    "    raise RuntimeError('assistant_model_api_key must be set in configs/simulation_config.json')\n",
    "\n",
    "client = Client(\n",
    "    assistant_model_url=CLIENT_ASSISTANT_MODEL_URL,\n",
    "    assistant_model_api_key=CLIENT_ASSISTANT_MODEL_API_KEY,\n",
    "    assistant_model_name=CLIENT_ASSISTANT_MODEL_NAME,\n",
    "    steer_api_key=CLIENT_STEER_API_KEY,\n",
    "    timeout=CLIENT_TIMEOUT,\n",
    "    max_retries=CLIENT_MAX_RETRIES,\n",
    "    rate_limit_retries=CLIENT_RATE_LIMIT_RETRIES,\n",
    ")\n",
    "\n",
    "runner = client.simulation_runner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ej6XeZcTuux"
   },
   "source": [
    "## Generate simulated user interactions\n",
    "\n",
    "Runs simulations and writes a JSONL dataset with conversation, assistant response, and steering persona.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4508,
     "status": "error",
     "timestamp": 1757615366646,
     "user": {
      "displayName": "Tsach Mackey",
      "userId": "06343373149479742516"
     },
     "user_tz": 420
    },
    "id": "h6Wb_3S8T8or",
    "outputId": "9a5d94cf-70b0-4f26-ad91-a320af07bcd3"
   },
   "outputs": [],
   "source": [
    "simulations = client.simulate(\n",
    "    steer_config=steer_config,\n",
    "    k=SIM_SAMPLES,\n",
    "    num_exchanges=SIM_EXCHANGES,\n",
    "    batch_delay=SIM_DELAY,\n",
    "    steer_temperature=SIM_STEER_TEMPERATURE,\n",
    "    steer_max_tokens=SIM_STEER_MAX_TOKENS,\n",
    "    mix_traits=SIM_MIX_TRAITS,\n",
    "    max_concurrency=SIM_MAX_CONCURRENCY,\n",
    ")\n",
    "\n",
    "rows = []\n",
    "for sim in simulations:\n",
    "    messages = list(sim.conv_prefix)\n",
    "    assistant_response = (sim.response or \"\").strip()\n",
    "\n",
    "    if _needs_fallback(assistant_response):\n",
    "        fallback = \"\"\n",
    "        cutoff_index = None\n",
    "        for idx in range(len(messages) - 1, -1, -1):\n",
    "            message = messages[idx]\n",
    "            if message.get(\"role\") == \"assistant\":\n",
    "                candidate = (message.get(\"content\") or \"\").strip()\n",
    "                if candidate and \"###STOP###\" not in candidate:\n",
    "                    fallback = candidate\n",
    "                    cutoff_index = idx\n",
    "                    break\n",
    "        if fallback:\n",
    "            assistant_response = fallback\n",
    "            if cutoff_index is not None:\n",
    "                messages = messages[: cutoff_index + 1]\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"conversation_messages\": messages,\n",
    "            \"assistant_response\": assistant_response,\n",
    "            \"steering_persona\": persona_from_steer(runner, getattr(sim, \"steer\", None)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "dataset_path = RESULTS_DIR / f\"{FILENAME_BASE}_{RUN_ID}_dataset.jsonl\"\n",
    "with dataset_path.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "    for row in rows:\n",
    "        convo_lines = conversation_lines(row[\"conversation_messages\"])\n",
    "        serializable = {\n",
    "            \"conversation\": \"\\n\".join(convo_lines),\n",
    "            \"assistant_response\": row[\"assistant_response\"],\n",
    "            \"steering_persona\": row[\"steering_persona\"],\n",
    "        }\n",
    "        fh.write(json.dumps(serializable, ensure_ascii=False))\n",
    "        fh.write(chr(10))\n",
    "print(f\"Saved {len(rows)} simulations to {dataset_path}\")\n",
    "\n",
    "for idx, row in enumerate(rows, start=1):\n",
    "    print()\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Conversation {idx}\")\n",
    "    print(\"-\" * 40)\n",
    "    persona = row[\"steering_persona\"] or {}\n",
    "    print(\"Persona:\")\n",
    "    print(json.dumps(persona, indent=2, ensure_ascii=False) if persona else \"  <none>\")\n",
    "    print()\n",
    "    print(\"Transcript:\")\n",
    "    turns = conversation_lines(row[\"conversation_messages\"])\n",
    "    for turn_no, line in enumerate(turns, start=1):\n",
    "        print(f\"{turn_no:02d}. {line}\")\n",
    "    print(f\"{len(turns) + 1:02d}. assistant: {row['assistant_response'] or '<no response>'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmcHKUAGUZoL"
   },
   "source": [
    "## Upload simulations as dataset and load judge model on Together\n",
    "\n",
    "Uploads the dataset to Together and starts a safety-score evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1360,
     "status": "error",
     "timestamp": 1757560922662,
     "user": {
      "displayName": "Quinten Lisowe",
      "userId": "05399425339750097468"
     },
     "user_tz": 300
    },
    "id": "f66S96r3UYd5",
    "outputId": "9c26c699-08e9-43db-f052-60483efb80f8"
   },
   "outputs": [],
   "source": [
    "together_client = together.Together(api_key=CLIENT_ASSISTANT_MODEL_API_KEY)\n",
    "\n",
    "upload = together_client.files.upload(file=str(dataset_path), purpose=TOGETHER_UPLOAD_PURPOSE)\n",
    "upload_id = getattr(upload, 'id', None)\n",
    "if upload_id is None:\n",
    "    upload_id = upload['id']\n",
    "\n",
    "requestor = api_requestor.APIRequestor(client=together_client.client)\n",
    "payload = {\n",
    "    'type': TOGETHER_EVAL_TYPE,\n",
    "    'parameters': {\n",
    "        'judge': {\n",
    "            'model': ASSESS_JUDGE_MODEL_NAME,\n",
    "            'model_source': TOGETHER_JUDGE_MODEL_SOURCE,\n",
    "            'system_template': JUDGE_SYSTEM_PROMPT,\n",
    "        },\n",
    "        'input_data_file_path': upload_id,\n",
    "        'model_to_evaluate': TOGETHER_MODEL_TO_EVALUATE,\n",
    "        'min_score': TOGETHER_MIN_SCORE,\n",
    "        'max_score': TOGETHER_MAX_SCORE,\n",
    "        'pass_threshold': TOGETHER_PASS_THRESHOLD,\n",
    "    },\n",
    "}\n",
    "response, _, _ = requestor.request(\n",
    "    options=TogetherRequest(method='POST', url='evaluation', params=payload),\n",
    "    stream=False,\n",
    ")\n",
    "evaluation = getattr(response, 'data', response)\n",
    "workflow_id = getattr(evaluation, 'workflow_id', evaluation['workflow_id'])\n",
    "print(f'Started evaluation {workflow_id}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5M1TgiNpVseN"
   },
   "source": [
    "## Eval results and analysis\n",
    "\n",
    "Polls for completion, enriches results with personas, and prints a summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEmq7n61WBOS"
   },
   "outputs": [],
   "source": [
    "deadline = time.time() + TOGETHER_POLL_TIMEOUT_SECONDS\n",
    "results_path = None\n",
    "while time.time() < deadline:\n",
    "    status_obj = together_client.evaluation.status(workflow_id)\n",
    "    status_raw = str(getattr(status_obj, \"status\", \"pending\"))\n",
    "    state = status_raw.lower().split('.')[-1]\n",
    "    print(f\"status: {status_raw}\")\n",
    "    if state in {\"completed\", \"success\", \"failed\", \"error\", \"user_error\"}:\n",
    "        results = getattr(status_obj, \"results\", None)\n",
    "        if isinstance(results, dict) and results.get(\"result_file_id\"):\n",
    "            results_path = RESULTS_DIR / f\"{FILENAME_BASE}_{RUN_ID}_{workflow_id}_results.jsonl\"\n",
    "            together_client.files.retrieve_content(results[\"result_file_id\"], output=str(results_path))\n",
    "            print(f\"Downloaded results to {results_path}\")\n",
    "            evaluation_rows = []\n",
    "            with results_path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "                for line in fh:\n",
    "                    text = line.strip()\n",
    "                    if text:\n",
    "                        evaluation_rows.append(json.loads(text))\n",
    "            for idx, row in enumerate(rows, start=1):\n",
    "                evaluation = evaluation_rows[idx - 1] if idx - 1 < len(evaluation_rows) else {}\n",
    "                print()\n",
    "                print(\"=\" * 40)\n",
    "                print(f\"Conversation {idx}\")\n",
    "                print(\"-\" * 40)\n",
    "                persona = row.get(\"steering_persona\") or {}\n",
    "                print(\"Persona:\")\n",
    "                print(json.dumps(persona, indent=2, ensure_ascii=False) if persona else \"  <none>\")\n",
    "                print()\n",
    "                print(\"Transcript:\")\n",
    "                turns = conversation_lines(row.get(\"conversation_messages\", []))\n",
    "                for turn_no, line in enumerate(turns, start=1):\n",
    "                    print(f\"{turn_no:02d}. {line}\")\n",
    "                print(f\"{len(turns) + 1:02d}. assistant: {row.get('assistant_response') or '<no response>'}\")\n",
    "                score = evaluation.get(\"score\", \"-\")\n",
    "                passed = evaluation.get(\"pass\")\n",
    "                rationale = evaluation.get(\"feedback\") or evaluation.get(\"rationale\")\n",
    "                print()\n",
    "                print(\"Assessment:\")\n",
    "                print(f\"  score: {score}\")\n",
    "                if passed is not None:\n",
    "                    print(f\"  pass: {passed}\")\n",
    "                if rationale:\n",
    "                    print(f\"  rationale: {rationale}\")\n",
    "        break\n",
    "    time.sleep(TOGETHER_POLL_INTERVAL_SECONDS)\n",
    "else:\n",
    "    print(\"Timed out waiting for evaluation to finish.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "collinear-synthetic",
   "language": "python",
   "name": "collinear-synthetic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
