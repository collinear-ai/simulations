{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMjptH4bTgRb"
   },
   "source": [
    "# High-Fidelity Agent Evaluation with TraitBasis + Together Evals\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to generate realistic, multi-turn user interactions using TraitBasis and evaluate AI agent responses with Together Evals. \n",
    "\n",
    "### Why do I need this?\n",
    "With different user personas, you can test your service from multiple perspectives and ensure every user gets high-quality responses, regardless of their traits.\n",
    "\n",
    "\n",
    "By the end of this tutorial, you'll understand how to:\n",
    "- Generate conversations with configurable user personas (traits like impatience, confusion, skepticism)\n",
    "- Run simulations with Together AI models as agents\n",
    "- Automatically evaluate agent performance using LLM-as-a-Judge\n",
    "- Analyze results with scores, pass/fail status, and rationales\n",
    "\n",
    "### What is TraitBasis?\n",
    "TraitBasis is a method for controllable generation of user interactions that maintains consistent personas across multiple turns without suffering from persona drift or intent loss in long contexts.\n",
    "\n",
    "### Workflow\n",
    "1. **Configure**: Set up agent model, TraitBasis personas, and evaluation parameters\n",
    "2. **Generate**: Create multi-turn conversations with realistic user behaviors\n",
    "3. **Evaluate**: Upload to Together Evals and judge agent responses\n",
    "4. **Analyze**: Review scores, rationales, and persona-specific performance\n",
    "\n",
    "## üöÄ Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4191,
     "status": "ok",
     "timestamp": 1757615218602,
     "user": {
      "displayName": "Tsach Mackey",
      "userId": "06343373149479742516"
     },
     "user_tz": 420
    },
    "id": "91_FEwA1RUyP"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import nest_asyncio\n",
    "import together\n",
    "from collinear.client import Client\n",
    "\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wdpf8OEuVIHF"
   },
   "source": [
    "## üõ†Ô∏è Utility Functions\n",
    "\n",
    "Helper functions for formatting conversations, extracting personas, and handling results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1757615218776,
     "user": {
      "displayName": "Tsach Mackey",
      "userId": "06343373149479742516"
     },
     "user_tz": 420
    },
    "id": "NW8ABbOTVNiz"
   },
   "outputs": [],
   "source": [
    "def conversation_lines(messages):\n",
    "    \"\"\"Format message list into readable lines with role prefixes.\"\"\"\n",
    "    lines = []\n",
    "    for message in messages:\n",
    "        role = message.get('role')\n",
    "        content = message.get('content')\n",
    "        if content:\n",
    "            lines.append(f\"{role}: {content}\")\n",
    "    return lines\n",
    "\n",
    "\n",
    "def conversation_text(messages):\n",
    "    \"\"\"Convert messages to plain text transcript.\"\"\"\n",
    "    return \"\\n\".join(conversation_lines(messages))\n",
    "\n",
    "\n",
    "def persona_from_traitmix(runner, traitmix):\n",
    "    \"\"\"Extract persona characteristics and traits from TraitMix config.\"\"\"\n",
    "    if not traitmix:\n",
    "        return {}\n",
    "    try:\n",
    "        characteristics = runner._user_characteristics_payload(traitmix)\n",
    "    except Exception:\n",
    "        characteristics = {}\n",
    "    return {\n",
    "        'characteristics': characteristics or {},\n",
    "        'traits': dict(getattr(traitmix, 'traits', {}) or {}),\n",
    "    }\n",
    "\n",
    "\n",
    "def print_evaluation_results(path: Path) -> None:\n",
    "    \"\"\"Parse and display evaluation results from JSONL file.\"\"\"\n",
    "    for idx, line in enumerate(path.read_text(encoding='utf-8').splitlines(), start=1):\n",
    "        try:\n",
    "            row = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"[{idx}] could not parse result\")\n",
    "            print(line)\n",
    "            continue\n",
    "        score = row.get('score', '-')\n",
    "        passed = row.get('pass')\n",
    "        rationale = row.get('feedback') or row.get('rationale')\n",
    "        print(f\"[{idx}] score={score} status={(passed if passed is not None else '-')}\")\n",
    "        if rationale:\n",
    "            print(f\"  rationale: {rationale}\")\n",
    "\n",
    "\n",
    "def _needs_fallback(response: str) -> bool:\n",
    "    \"\"\"Check if agent response is empty, error, or stop signal.\"\"\"\n",
    "    if not response:\n",
    "        return True\n",
    "    stripped = response.strip()\n",
    "    if not stripped:\n",
    "        return True\n",
    "    if stripped == \"###STOP###\":\n",
    "        return True\n",
    "    if stripped.lower().startswith(\"assistant returned empty response\"):\n",
    "        return True\n",
    "    if stripped.lower().startswith(\"error:\"):\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóùÔ∏è How to Get API Keys for Together AI and Collinear\n",
    "\n",
    "To run these simulations, you'll need API keys for both Together AI and Collinear.\n",
    "\n",
    "**Together AI:**\n",
    "1. **Register for a Together AI account:**  \n",
    "   Visit [Together AI](https://www.together.ai/) and sign up for a free account.\n",
    "2. **Get your API key:**  \n",
    "   After logging in, navigate to your account dashboard to find your API key.\n",
    "3. **Set the environment variable:**  \n",
    "   Make sure your API key is set in your shell environment as `TOGETHER_API_KEY`.  \n",
    "   For example, in bash/zsh:\n",
    "   ```\n",
    "   export TOGETHER_API_KEY=\"your-together-api-key-here\"\n",
    "   export TOGETHER_BASE_URL=\"https://api.together.xyz/v1\"\n",
    "   ```\n",
    "\n",
    "**Collinear:**\n",
    "1. **Register for a Collinear AI account:**\n",
    "   - Visit [Collinear AI](https://platform.collinear.ai/) and sign up for a free account.\n",
    "2. **Access your API key:**\n",
    "   - After logging in, navigate to the **Developer** menu in the bottom-left corner of the dashboard.\n",
    "   - Copy your API key from there.\n",
    "3. **Set the environment variable:**\n",
    "   ```\n",
    "   export COLLINEAR_API_KEY=\"your-collinear-api-key-here\"\n",
    "   ```\n",
    "\n",
    "You'll need to have both API keys set (or configured in `configs/simulation_config.json`) before starting the simulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Configure Your Models\n",
    "\n",
    "Before running the simulation, you need to configure the models in `configs/simulation_config.json`:\n",
    "\n",
    "1. **`assistant_model_name`**: The model that will act as your assistant/agent in the conversations.\n",
    "   - Default: `\"openai/gpt-oss-20b\"`\n",
    "   - This should be a model available through Together AI's serverless API: https://docs.together.ai/docs/serverless-models\n",
    "\n",
    "2. **`judge_model_name`**: The model that will evaluate the quality of the assistant's responses.\n",
    "   - Default: `\"\"deepseek-ai/DeepSeek-V3.1\"\"`\n",
    "   - This model should also be from Together AI's available models: https://docs.together.ai/docs/serverless-models\n",
    "\n",
    "Make sure both models are set correctly in your configuration file before proceeding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWBXYPgGTI43"
   },
   "source": [
    "## ‚öôÔ∏è Load Configuration\n",
    "\n",
    "Loads simulation parameters, TraitMix persona configs, and Together Evals settings from JSON files.\n",
    "\n",
    "**Key Configuration Sections:**\n",
    "- `client`: Agent model, API keys, and connection settings\n",
    "- `simulate`: Number of samples, conversation turns, temperature, concurrency\n",
    "- `assess`: Judge model configuration\n",
    "- `together`: Evaluation type, scoring thresholds, output paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_DIR = Path('configs')\n",
    "SIMULATION_CONFIG_FILE = CONFIG_DIR / 'simulation_config.json'\n",
    "config = json.loads(SIMULATION_CONFIG_FILE.read_text())\n",
    "\n",
    "# Load TraitMix persona configuration\n",
    "traitmix_name = config.get('traitmix_config_file', 'traitmix_config_airline.json')\n",
    "TRAITMIX_CONFIG_FILE = CONFIG_DIR / Path(traitmix_name).name\n",
    "traitmix_config = json.loads(TRAITMIX_CONFIG_FILE.read_text())\n",
    "TRAITMIX_TASKS = traitmix_config.get('tasks') or []\n",
    "\n",
    "# Agent model and API settings\n",
    "client_cfg = config.get('client', {}) or {}\n",
    "CLIENT_ASSISTANT_MODEL_URL = client_cfg.get('assistant_model_url', 'https://api.together.xyz/v1')\n",
    "CLIENT_ASSISTANT_MODEL_API_KEY = client_cfg.get('assistant_model_api_key')\n",
    "CLIENT_ASSISTANT_MODEL_NAME = client_cfg.get('assistant_model_name', 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo')\n",
    "CLIENT_COLLINEAR_API_KEY = client_cfg.get('collinear_api_key', 'demo-001')\n",
    "CLIENT_TIMEOUT = float(client_cfg.get('timeout', 120))\n",
    "CLIENT_MAX_RETRIES = int(client_cfg.get('max_retries', 3))\n",
    "CLIENT_RATE_LIMIT_RETRIES = int(client_cfg.get('rate_limit_retries', 6))\n",
    "\n",
    "# Simulation parameters\n",
    "sim_cfg = config.get('simulate', {}) or {}\n",
    "SIM_SAMPLES = sim_cfg.get('k', 3)  # Number of conversations to generate\n",
    "SIM_EXCHANGES = sim_cfg.get('num_exchanges', 2)  # Turns per conversation\n",
    "SIM_DELAY = sim_cfg.get('batch_delay', 0.2)\n",
    "SIM_TRAITMIX_TEMPERATURE = sim_cfg.get('traitmix_temperature', 0.7)\n",
    "SIM_TRAITMIX_MAX_TOKENS = sim_cfg.get('traitmix_max_tokens', 256)\n",
    "SIM_MIX_TRAITS = bool(sim_cfg.get('mix_traits', False))  # Combine multiple traits\n",
    "SIM_MAX_CONCURRENCY = int(sim_cfg.get('max_concurrency', 8))\n",
    "\n",
    "# Judge model for evaluation\n",
    "assess_cfg = config.get('assess', {}) or {}\n",
    "ASSESS_JUDGE_MODEL_NAME = assess_cfg.get('judge_model_name')\n",
    "\n",
    "# Together Evals configuration\n",
    "together_cfg = config.get('together', {}) or {}\n",
    "RESULTS_DIR = Path(together_cfg.get('output_directory', '.')).joinpath('results')\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "JUDGE_SYSTEM_PROMPT = Path(together_cfg.get('judge_system_prompt', 'configs/judge_system_prompt.jinja')).read_text(encoding='utf-8')\n",
    "TOGETHER_UPLOAD_PURPOSE = together_cfg.get('upload_purpose', 'eval')\n",
    "TOGETHER_EVAL_TYPE = together_cfg.get('evaluation_type', 'score')\n",
    "TOGETHER_MODEL_TO_EVALUATE = together_cfg.get('model_to_evaluate', 'assistant_response')\n",
    "TOGETHER_JUDGE_MODEL_SOURCE = together_cfg.get('judge_model_source', 'serverless')\n",
    "TOGETHER_MIN_SCORE = together_cfg.get('min_score', 1.0)\n",
    "TOGETHER_MAX_SCORE = together_cfg.get('max_score', 10.0)\n",
    "TOGETHER_PASS_THRESHOLD = together_cfg.get('pass_threshold', 7.0)\n",
    "TOGETHER_POLL_TIMEOUT_SECONDS = int(together_cfg.get('poll_timeout_seconds', 300))\n",
    "TOGETHER_POLL_INTERVAL_SECONDS = int(together_cfg.get('poll_interval_seconds', 5))\n",
    "raw_prefix = together_cfg.get('results_filename_prefix') or together_cfg.get('output_filename') or 'together_eval'\n",
    "FILENAME_BASE = (str(raw_prefix).rsplit('.', 1)[0]).rstrip('_')\n",
    "RUN_ID = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print(f'Loaded simulation config {SIMULATION_CONFIG_FILE}')\n",
    "print(f'TraitMix config {TRAITMIX_CONFIG_FILE} | tasks: {TRAITMIX_TASKS or \"<none>\"}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Understanding TraitMix Configuration\n",
    "\n",
    "The TraitMix config defines user personas and behaviors for simulations.\n",
    "\n",
    "**Key Components:**\n",
    "- **Demographics**: Ages, genders, occupations, locations, languages\n",
    "- **Traits**: Behavioral characteristics with intensity levels (0-2)\n",
    "  - `impatience`: User wants quick answers, gets frustrated with delays\n",
    "  - `confusion`: User struggles to articulate needs clearly\n",
    "  - `skeptical`: User questions or doubts agent responses\n",
    "  - `incoherence`: User messages lack clarity or logical flow\n",
    "- **Intents**: User goals (e.g., track order, apply promo code, return item)\n",
    "- **Tasks**: Domain-specific scenarios (e.g., \"retail support\", \"airline booking\")\n",
    "\n",
    "TraitBasis samples from these distributions to create diverse, realistic user interactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables for API access\n",
    "if CLIENT_ASSISTANT_MODEL_API_KEY:\n",
    "    os.environ['TOGETHER_API_KEY'] = CLIENT_ASSISTANT_MODEL_API_KEY\n",
    "if CLIENT_ASSISTANT_MODEL_URL:\n",
    "    os.environ['TOGETHER_BASE_URL'] = CLIENT_ASSISTANT_MODEL_URL\n",
    "if CLIENT_COLLINEAR_API_KEY:\n",
    "    os.environ['COLLINEAR_API_KEY'] = CLIENT_COLLINEAR_API_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîå Initialize Client\n",
    "\n",
    "Creates the Collinear client with configured agent model and TraitBasis API credentials.\n",
    "\n",
    "The client handles:\n",
    "- Connection to your agent model (e.g., Together AI)\n",
    "- TraitBasis API for persona-driven user simulation\n",
    "- Rate limiting and retry logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1757615357703,
     "user": {
      "displayName": "Tsach Mackey",
      "userId": "06343373149479742516"
     },
     "user_tz": 420
    },
    "id": "9gUY0rdqSov5"
   },
   "outputs": [],
   "source": [
    "if not CLIENT_ASSISTANT_MODEL_API_KEY:\n",
    "    raise RuntimeError('assistant_model_api_key must be set in configs/simulation_config.json')\n",
    "\n",
    "client = Client(\n",
    "    assistant_model_url=CLIENT_ASSISTANT_MODEL_URL,\n",
    "    assistant_model_api_key=CLIENT_ASSISTANT_MODEL_API_KEY,\n",
    "    assistant_model_name=CLIENT_ASSISTANT_MODEL_NAME,\n",
    "    collinear_api_key=CLIENT_COLLINEAR_API_KEY,\n",
    "    timeout=CLIENT_TIMEOUT,\n",
    "    max_retries=CLIENT_MAX_RETRIES,\n",
    "    rate_limit_retries=CLIENT_RATE_LIMIT_RETRIES,\n",
    ")\n",
    "\n",
    "runner = client.simulation_runner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ej6XeZcTuux"
   },
   "source": [
    "## üí¨ Generate Simulated Conversations\n",
    "\n",
    "Runs TraitBasis simulations to create realistic multi-turn conversations between users and your agent.\n",
    "\n",
    "**What Happens Here:**\n",
    "1. TraitBasis generates user messages with configured personas (traits, demographics, intents)\n",
    "2. Your agent model responds to each user message\n",
    "3. Conversations continue for the specified number of exchanges\n",
    "4. Results are saved as JSONL with conversation history, agent response, and persona details\n",
    "\n",
    "**Fallback Logic:**\n",
    "If the agent returns an empty or error response, the code extracts the last valid assistant message from the conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4508,
     "status": "error",
     "timestamp": 1757615366646,
     "user": {
      "displayName": "Tsach Mackey",
      "userId": "06343373149479742516"
     },
     "user_tz": 420
    },
    "id": "h6Wb_3S8T8or",
    "outputId": "9a5d94cf-70b0-4f26-ad91-a320af07bcd3"
   },
   "outputs": [],
   "source": [
    "# Generate multi-turn conversations with TraitBasis personas\n",
    "simulations = client.simulate(\n",
    "    traitmix_config=traitmix_config,\n",
    "    k=SIM_SAMPLES,\n",
    "    num_exchanges=SIM_EXCHANGES,\n",
    "    batch_delay=SIM_DELAY,\n",
    "    traitmix_temperature=SIM_TRAITMIX_TEMPERATURE,\n",
    "    traitmix_max_tokens=SIM_TRAITMIX_MAX_TOKENS,\n",
    "    mix_traits=SIM_MIX_TRAITS,\n",
    "    max_concurrency=SIM_MAX_CONCURRENCY,\n",
    ")\n",
    "\n",
    "# Process simulation results and handle fallback responses\n",
    "rows = []\n",
    "for sim in simulations:\n",
    "    messages = list(sim.conv_prefix)\n",
    "    assistant_response = (sim.response or \"\").strip()\n",
    "\n",
    "    # If final response is empty/error, use last valid assistant message\n",
    "    if _needs_fallback(assistant_response):\n",
    "        fallback = \"\"\n",
    "        cutoff_index = None\n",
    "        # Search backwards for last valid assistant response\n",
    "        for idx in range(len(messages) - 1, -1, -1):\n",
    "            message = messages[idx]\n",
    "            if message.get(\"role\") == \"assistant\":\n",
    "                candidate = (message.get(\"content\") or \"\").strip()\n",
    "                if candidate and \"###STOP###\" not in candidate:\n",
    "                    fallback = candidate\n",
    "                    cutoff_index = idx\n",
    "                    break\n",
    "        if fallback:\n",
    "            assistant_response = fallback\n",
    "            if cutoff_index is not None:\n",
    "                messages = messages[: cutoff_index + 1]\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"conversation_messages\": messages,\n",
    "            \"assistant_response\": assistant_response,\n",
    "            \"traitmix_persona\": persona_from_traitmix(runner, getattr(sim, \"traitmix\", None)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Save dataset as JSONL for Together Evals\n",
    "dataset_path = RESULTS_DIR / f\"{FILENAME_BASE}_{RUN_ID}_dataset.jsonl\"\n",
    "with dataset_path.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "    for row in rows:\n",
    "        convo_lines = conversation_lines(row[\"conversation_messages\"])\n",
    "        serializable = {\n",
    "            \"conversation\": \"\\n\".join(convo_lines),\n",
    "            \"assistant_response\": row[\"assistant_response\"],\n",
    "            \"traitmix_persona\": row[\"traitmix_persona\"],\n",
    "        }\n",
    "        fh.write(json.dumps(serializable, ensure_ascii=False))\n",
    "        fh.write(chr(10))\n",
    "print(f\"Saved {len(rows)} simulations to {dataset_path}\")\n",
    "\n",
    "for idx, row in enumerate(rows[:5], start=1):\n",
    "    print()\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Conversation {idx}\")\n",
    "    print(\"-\" * 40)\n",
    "    persona = row[\"traitmix_persona\"] or {}\n",
    "    print(\"Persona:\")\n",
    "    print(json.dumps(persona, indent=2, ensure_ascii=False) if persona else \"  <none>\")\n",
    "    print()\n",
    "    print(\"Transcript:\")\n",
    "    turns = conversation_lines(row[\"conversation_messages\"])\n",
    "    for turn_no, line in enumerate(turns, start=1):\n",
    "        print(f\"{turn_no:02d}. {line}\")\n",
    "    print(f\"{len(turns) + 1:02d}. assistant: {row['assistant_response'] or '<no response>'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmcHKUAGUZoL"
   },
   "source": [
    "## üì§ Upload Dataset & Start Evaluation\n",
    "\n",
    "Uploads the conversation dataset to Together AI and creates an evaluation job.\n",
    "\n",
    "**Evaluation Configuration:**\n",
    "- **Judge Model**: The LLM that evaluates agent responses\n",
    "- **Evaluation Type**: `score` (rates responses on a scale)\n",
    "- **Score Range**: Min and max scores (e.g., 1-10)\n",
    "- **Pass Threshold**: Minimum score considered successful\n",
    "- **Judge System Prompt**: Custom instructions for the judge model. References the conversation via `{{conversation}}` template variable\n",
    "\n",
    "- **Model to Evaluate**: Set to `assistant_response` (a column in our dataset containing pre-generated responses)\n",
    "- **Note**: We use specialized Collinear models for generation before evaluation, rather than having Together generate responses during evaluation\n",
    "\n",
    "The evaluation job runs asynchronously on Together's infrastructure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1360,
     "status": "error",
     "timestamp": 1757560922662,
     "user": {
      "displayName": "Quinten Lisowe",
      "userId": "05399425339750097468"
     },
     "user_tz": 300
    },
    "id": "f66S96r3UYd5",
    "outputId": "9c26c699-08e9-43db-f052-60483efb80f8"
   },
   "outputs": [],
   "source": [
    "together_client = together.Together(api_key=CLIENT_ASSISTANT_MODEL_API_KEY)\n",
    "\n",
    "# Upload dataset to Together AI\n",
    "upload = together_client.files.upload(file=str(dataset_path), purpose=TOGETHER_UPLOAD_PURPOSE)\n",
    "upload_id = getattr(upload, 'id', None)\n",
    "if upload_id is None:\n",
    "    upload_id = upload['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_job = together_client.evaluation.create(\n",
    "    type=TOGETHER_EVAL_TYPE,\n",
    "    # Pass the detailed configuration object\n",
    "    # We are evaluating the field 'assistant_response' with responses that we generated earlier.\n",
    "    model_to_evaluate='assistant_response',\n",
    "    input_data_file_path=upload_id,\n",
    "    # Judge model details, it is better to use as strong model as possible\n",
    "    judge_model=ASSESS_JUDGE_MODEL_NAME,\n",
    "    judge_model_source=TOGETHER_JUDGE_MODEL_SOURCE,\n",
    "    judge_system_template=JUDGE_SYSTEM_PROMPT,\n",
    "    min_score=TOGETHER_MIN_SCORE,\n",
    "    max_score=TOGETHER_MAX_SCORE,\n",
    "    pass_threshold=TOGETHER_PASS_THRESHOLD\n",
    ")\n",
    "\n",
    "workflow_id = evaluation_job.workflow_id\n",
    "print(f'Started evaluation {workflow_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5M1TgiNpVseN"
   },
   "source": [
    "## üìä Poll Results & Analysis\n",
    "\n",
    "Waits for the evaluation to complete, downloads results, and displays detailed analysis.\n",
    "\n",
    "**What You'll See:**\n",
    "- Evaluation status updates during polling\n",
    "- Per-conversation breakdown with:\n",
    "  - **Persona**: Traits and characteristics for each simulated user\n",
    "  - **Transcript**: Full conversation history\n",
    "  - **Assessment**: Score, pass/fail status, and judge rationale\n",
    "\n",
    "This allows you to understand how your agent performs across different user personas and identify areas for improvement.\n",
    "\n",
    "You can also see your jobs statuses at https://api.together.ai/evaluations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEmq7n61WBOS"
   },
   "outputs": [],
   "source": [
    "# Poll for evaluation completion\n",
    "deadline = time.time() + TOGETHER_POLL_TIMEOUT_SECONDS\n",
    "results_path = None\n",
    "while time.time() < deadline:\n",
    "    status_obj = together_client.evaluation.status(workflow_id)\n",
    "    status_raw = str(getattr(status_obj, \"status\", \"pending\"))\n",
    "    state = status_raw.lower().split('.')[-1]\n",
    "    print(f\"status: {status_raw}\")\n",
    "    \n",
    "    # Check if evaluation is complete\n",
    "    if state in {\"completed\", \"success\", \"failed\", \"error\", \"user_error\"}:\n",
    "        results = getattr(status_obj, \"results\", None)\n",
    "        if isinstance(results, dict) and results.get(\"result_file_id\"):\n",
    "            # Download results file\n",
    "            results_path = RESULTS_DIR / f\"{FILENAME_BASE}_{RUN_ID}_{workflow_id}_results.jsonl\"\n",
    "            together_client.files.retrieve_content(results[\"result_file_id\"], output=str(results_path))\n",
    "            print(f\"Downloaded results to {results_path}\")\n",
    "            \n",
    "            # Parse evaluation results\n",
    "            evaluation_rows = []\n",
    "            with results_path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "                for line in fh:\n",
    "                    text = line.strip()\n",
    "                    if text:\n",
    "                        evaluation_rows.append(json.loads(text))\n",
    "            \n",
    "            # Display results with personas and transcripts\n",
    "            for idx, row in enumerate(rows, start=1):\n",
    "                evaluation = evaluation_rows[idx - 1] if idx - 1 < len(evaluation_rows) else {}\n",
    "                print()\n",
    "                print(\"=\" * 40)\n",
    "                print(f\"Conversation {idx}\")\n",
    "                print(\"-\" * 40)\n",
    "                persona = row.get(\"traitmix_persona\") or {}\n",
    "                print(\"Persona:\")\n",
    "                print(json.dumps(persona, indent=2, ensure_ascii=False) if persona else \"  <none>\")\n",
    "                print()\n",
    "                print(\"Transcript:\")\n",
    "                turns = conversation_lines(row.get(\"conversation_messages\", []))\n",
    "                for turn_no, line in enumerate(turns, start=1):\n",
    "                    print(f\"{turn_no:02d}. {line}\")\n",
    "                print(f\"{len(turns) + 1:02d}. assistant: {row.get('assistant_response') or '<no response>'}\")\n",
    "                score = evaluation.get(\"score\", \"-\")\n",
    "                passed = evaluation.get(\"pass\")\n",
    "                rationale = evaluation.get(\"feedback\") or evaluation.get(\"rationale\")\n",
    "                print()\n",
    "                print(\"Assessment:\")\n",
    "                print(f\"  score: {score}\")\n",
    "                if passed is not None:\n",
    "                    print(f\"  pass: {passed}\")\n",
    "                if rationale:\n",
    "                    print(f\"  rationale: {rationale}\")\n",
    "        break\n",
    "    time.sleep(TOGETHER_POLL_INTERVAL_SECONDS)\n",
    "else:\n",
    "    print(\"Timed out waiting for evaluation to finish.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for some user types the assistant score is higher than for others. This information can be used to create improved assistant responses for these examples and fine-tune the model to improve on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = together_client.evaluation.status(workflow_id).results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results Summary\n",
    "\n",
    "The `results` object contains aggregated evaluation metrics:\n",
    "- `mean_score`: Average score across all evaluated conversations (1-5 scale)\n",
    "- `pass_percentage`: Percentage of conversations that met the pass threshold (‚â•4.0)\n",
    "- `std_score`: Standard deviation of scores, indicating consistency\n",
    "- `failed_samples`: Number of samples that failed to evaluate\n",
    "- `generation_fail_count`: Number of failures during response generation\n",
    "- `invalid_score_count`: Number of evaluations with invalid scores\n",
    "- `judge_fail_count`: Number of failures during judge evaluation\n",
    "- `result_file_id`: File ID containing detailed per-conversation results with judge feedback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Generate synthetic user-assistant conversations using TraitMix personas\n",
    "2. Evaluate assistant performance using Together AI's evaluation platform\n",
    "3. Analyze results to understand system behavior across diverse user types\n",
    "\n",
    "By simulating conversations with varied user personas and systematically evaluating responses,\n",
    "you can assess how your AI systems will behave for different users, track improvements over time,\n",
    "and gain confidence in your results before deploying to production. This workflow enables data-driven\n",
    "iteration on prompts, models, and system design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "collinear-synthetic",
   "language": "python",
   "name": "collinear-synthetic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
